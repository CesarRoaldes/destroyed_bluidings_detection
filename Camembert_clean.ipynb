{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM0bIG2iDwe9bfkp4/Wvt90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "392f4de4ea15400c805ee9302c4d6e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3c066cd5ed7d4b40ba51c5a38aad1686",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbb862173c314601a77db06b53680e8f",
              "IPY_MODEL_ef111cc29d894ea0a9453b002f270686"
            ]
          }
        },
        "3c066cd5ed7d4b40ba51c5a38aad1686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbb862173c314601a77db06b53680e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cccd0184e0004943af1fa3b870954a28",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 810912,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 810912,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aea8e499924c4ca6a26d57c7cbd64fa1"
          }
        },
        "ef111cc29d894ea0a9453b002f270686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b909758c6ffb463eb860a4a9d2aeb2ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 811k/811k [00:37&lt;00:00, 21.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6725d08e2a245b0b1e684ba43de5519"
          }
        },
        "cccd0184e0004943af1fa3b870954a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aea8e499924c4ca6a26d57c7cbd64fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b909758c6ffb463eb860a4a9d2aeb2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6725d08e2a245b0b1e684ba43de5519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CesarRoaldes/satellite_imagery/blob/master/Camembert_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# BERT classification model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "ae0fdcc5-850b-40b5-d2de-cd6e6fb786b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "84ae3fdc-d216-48fd-8ca5-1150fe293ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "d8c115c0-a605-4993-a701-3d009ffa50d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "20bb077d-3a05-448d-c5fd-6b0e3ebc82a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 2.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 18.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 20.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=72f95e283adfea8987687976a57b25e4ca114d7743e61219f88e3a42cadd62e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Import medium_df_desq in \"files\"\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRDpDKe9_j-",
        "colab_type": "text"
      },
      "source": [
        "1. Unbalanced sample \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Gl1QBWAlTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrqgCPYFApFQ",
        "colab_type": "text"
      },
      "source": [
        "2. Balanced sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOIKxWaXHPVB",
        "colab_type": "code",
        "outputId": "8d539235-2a32-48b2-e53e-10fff8b678c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mesdames, Messieurs,Après avoir salué une nouv...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M. le président. L'ordre du jour appelle le dé...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Madame et messieurs les ministres,Mesdames, me...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M. le président. L'ordre du jour appelle le dé...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Monsieur le maire, mon cher Gilbert,Monsieur ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  Mesdames, Messieurs,Après avoir salué une nouv...     0\n",
              "1  M. le président. L'ordre du jour appelle le dé...     0\n",
              "2  Madame et messieurs les ministres,Mesdames, me...     1\n",
              "3  M. le président. L'ordre du jour appelle le dé...     1\n",
              "4   Monsieur le maire, mon cher Gilbert,Monsieur ...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNHgnNbAsoq",
        "colab_type": "text"
      },
      "source": [
        "3. Spliting texts in order to feed the model with all parts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BepFr9T7A0f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_text=[]\n",
        "for text in df.Texte[0:1]:\n",
        "  token_text= text.split(' ')\n",
        "  for i in range(0,len(token_text),500):\n",
        "    split_text = split_text +[j for j in token_text[i:i+500]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JmisLKH2L8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1db990b-e2e7-4c96-ee57-5416f89f0550"
      },
      "source": [
        "split_text"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Messieurs,Je',\n",
              " 'suis',\n",
              " 'heureux',\n",
              " 'de',\n",
              " 'vous',\n",
              " 'saluer.',\n",
              " 'Quand',\n",
              " 'je',\n",
              " 'dis',\n",
              " 'que',\n",
              " 'je',\n",
              " 'suis',\n",
              " 'heureux',\n",
              " 'de',\n",
              " 'vous',\n",
              " 'saluer,',\n",
              " 'ce',\n",
              " \"n'est\",\n",
              " 'pas',\n",
              " 'une',\n",
              " 'simple',\n",
              " 'affirmation',\n",
              " 'de',\n",
              " 'politesse.',\n",
              " 'Je',\n",
              " 'le',\n",
              " 'disais',\n",
              " 'à',\n",
              " \"l'instant\",\n",
              " 'à',\n",
              " 'Monsieur',\n",
              " 'le',\n",
              " 'Ministre',\n",
              " 'de',\n",
              " 'la',\n",
              " 'Défense,',\n",
              " 'M.',\n",
              " 'Richard,',\n",
              " \"c'est\",\n",
              " 'pour',\n",
              " 'moi',\n",
              " 'un',\n",
              " 'instant',\n",
              " 'où',\n",
              " 'il',\n",
              " 'y',\n",
              " 'a',\n",
              " 'un',\n",
              " 'peu',\n",
              " \"d'émotion;\",\n",
              " 'je',\n",
              " 'vais',\n",
              " 'vous',\n",
              " 'dire',\n",
              " 'pourquoi.Vous',\n",
              " 'êtes',\n",
              " 'la',\n",
              " 'première',\n",
              " 'classe',\n",
              " \"d'âge\",\n",
              " 'qui',\n",
              " 'ne',\n",
              " 'fera',\n",
              " 'pas',\n",
              " 'de',\n",
              " 'service',\n",
              " 'militaire.',\n",
              " \"C'est\",\n",
              " 'une',\n",
              " 'décision',\n",
              " 'que',\n",
              " \"j'ai\",\n",
              " 'prise,',\n",
              " 'il',\n",
              " 'y',\n",
              " 'a',\n",
              " 'deux',\n",
              " 'ans,',\n",
              " 'après',\n",
              " 'une',\n",
              " 'vraie',\n",
              " 'réflexion',\n",
              " 'et',\n",
              " 'un',\n",
              " 'vrai',\n",
              " 'débat.',\n",
              " 'Après',\n",
              " 'tout,',\n",
              " 'le',\n",
              " 'service',\n",
              " 'militaire',\n",
              " \"c'est\",\n",
              " 'une',\n",
              " 'vieille',\n",
              " 'tradition',\n",
              " 'nationale,',\n",
              " 'il',\n",
              " 'était',\n",
              " 'plus',\n",
              " 'que',\n",
              " 'centenaire.',\n",
              " 'Il',\n",
              " 'y',\n",
              " 'avait',\n",
              " 'toutes',\n",
              " 'sortes',\n",
              " 'de',\n",
              " 'raisons',\n",
              " 'à',\n",
              " 'cela,',\n",
              " 'notamment',\n",
              " 'la',\n",
              " 'nécessité',\n",
              " \"d'avoir\",\n",
              " 'une',\n",
              " 'armée',\n",
              " 'nombreuse',\n",
              " 'et',\n",
              " 'donc',\n",
              " \"d'avoir\",\n",
              " 'des',\n",
              " 'jeunes',\n",
              " 'formés',\n",
              " 'aux',\n",
              " 'combats,',\n",
              " 'à',\n",
              " \"l'utilisation\",\n",
              " 'des',\n",
              " 'armes',\n",
              " 'de',\n",
              " \"l'époque.On\",\n",
              " 'pouvait',\n",
              " \"s'interroger\",\n",
              " 'sur',\n",
              " 'la',\n",
              " 'nécessité',\n",
              " 'de',\n",
              " 'poursuivre',\n",
              " 'dans',\n",
              " 'cette',\n",
              " 'voie.',\n",
              " 'Il',\n",
              " 'y',\n",
              " 'avait',\n",
              " 'naturellement',\n",
              " 'des',\n",
              " 'critiques,',\n",
              " 'il',\n",
              " 'y',\n",
              " 'avait',\n",
              " 'beaucoup',\n",
              " 'de',\n",
              " 'jeunes',\n",
              " 'qui',\n",
              " 'se',\n",
              " 'disaient',\n",
              " \"qu'ils\",\n",
              " 'perdaient',\n",
              " 'un',\n",
              " 'peu',\n",
              " 'leur',\n",
              " 'temps,',\n",
              " \"d'autres\",\n",
              " 'qui',\n",
              " 'étaient',\n",
              " 'satisfaits.',\n",
              " 'Mais',\n",
              " 'il',\n",
              " 'y',\n",
              " 'avait',\n",
              " 'surtout',\n",
              " 'ceux',\n",
              " 'qui,',\n",
              " 'dans',\n",
              " 'notre',\n",
              " 'pays,',\n",
              " 'disaient:',\n",
              " 'le',\n",
              " 'service',\n",
              " 'militaire',\n",
              " \"c'est\",\n",
              " 'nécessaire',\n",
              " 'pour',\n",
              " 'la',\n",
              " 'cohésion',\n",
              " 'nationale,',\n",
              " \"c'est\",\n",
              " 'un',\n",
              " 'moyen',\n",
              " 'de',\n",
              " 'mettre',\n",
              " 'ensemble',\n",
              " 'les',\n",
              " 'garçons',\n",
              " 'issus',\n",
              " 'de',\n",
              " 'milieux',\n",
              " 'et',\n",
              " \"d'origines\",\n",
              " 'géographiques,',\n",
              " 'culturelles,',\n",
              " 'sociales,',\n",
              " 'différents',\n",
              " 'et',\n",
              " 'de',\n",
              " 'faire',\n",
              " 'en',\n",
              " 'sorte',\n",
              " \"qu'ils\",\n",
              " 'se',\n",
              " 'connaissent,',\n",
              " \"qu'ils\",\n",
              " 'soient',\n",
              " 'ensemble,',\n",
              " 'et',\n",
              " 'ce',\n",
              " 'que',\n",
              " \"l'on\",\n",
              " 'appelait',\n",
              " 'la',\n",
              " 'fraternité',\n",
              " 'des',\n",
              " 'casernes',\n",
              " 'était',\n",
              " 'considérée',\n",
              " 'par',\n",
              " 'beaucoup',\n",
              " 'comme',\n",
              " 'un',\n",
              " 'élément',\n",
              " 'important',\n",
              " 'de',\n",
              " 'la',\n",
              " 'cohésion',\n",
              " 'nationale.',\n",
              " 'Cet',\n",
              " 'argument',\n",
              " 'était',\n",
              " 'celui',\n",
              " 'de',\n",
              " 'beaucoup',\n",
              " 'de',\n",
              " 'conservateurs.',\n",
              " 'Les',\n",
              " 'conservateurs',\n",
              " 'ont',\n",
              " 'toujours',\n",
              " 'des',\n",
              " 'arguments',\n",
              " 'forts,',\n",
              " 'il',\n",
              " 'faut',\n",
              " 'en',\n",
              " 'tenir',\n",
              " 'compte.',\n",
              " \"D'autre\",\n",
              " 'part,',\n",
              " 'il',\n",
              " \"m'était\",\n",
              " 'apparu',\n",
              " 'que',\n",
              " 'tout',\n",
              " 'cela,',\n",
              " 'au',\n",
              " 'fond,',\n",
              " 'appartenait',\n",
              " 'un',\n",
              " 'peu',\n",
              " 'à',\n",
              " 'une',\n",
              " 'conception',\n",
              " 'dépassée',\n",
              " 'de',\n",
              " 'la',\n",
              " 'défense,',\n",
              " 'et',\n",
              " 'faisait',\n",
              " 'peser',\n",
              " 'au',\n",
              " 'total',\n",
              " 'sur',\n",
              " 'les',\n",
              " 'jeunes',\n",
              " 'une',\n",
              " 'contrainte',\n",
              " 'obligatoire',\n",
              " 'et',\n",
              " 'excessive',\n",
              " 'qui',\n",
              " \"n'était\",\n",
              " 'pas',\n",
              " 'justifiée,',\n",
              " 'ou',\n",
              " 'qui',\n",
              " 'ne',\n",
              " \"l'était\",\n",
              " \"plus.L'armée\",\n",
              " 'moderne',\n",
              " \"n'est\",\n",
              " 'naturellement',\n",
              " 'pas',\n",
              " 'celle',\n",
              " 'dont',\n",
              " 'on',\n",
              " 'avait',\n",
              " 'besoin',\n",
              " 'en',\n",
              " '1914',\n",
              " 'ou',\n",
              " '1939,',\n",
              " 'ou',\n",
              " 'même',\n",
              " 'après,',\n",
              " 'au',\n",
              " 'moment',\n",
              " 'des',\n",
              " 'guerres',\n",
              " 'de',\n",
              " 'décolonisation',\n",
              " 'en',\n",
              " 'Algérie',\n",
              " 'ou',\n",
              " 'autres.',\n",
              " 'Tout',\n",
              " 'cela',\n",
              " 'justifiait',\n",
              " 'que',\n",
              " \"l'on\",\n",
              " 'passe',\n",
              " '1',\n",
              " 'an',\n",
              " 'ou',\n",
              " '2,',\n",
              " 'voire',\n",
              " 'plus,',\n",
              " 'de',\n",
              " 'service',\n",
              " 'militaire,',\n",
              " '-',\n",
              " 'parfois',\n",
              " '3',\n",
              " 'ans',\n",
              " 'au',\n",
              " 'début',\n",
              " 'du',\n",
              " 'siècle,',\n",
              " '2',\n",
              " 'ans',\n",
              " 'et',\n",
              " 'demi',\n",
              " 'pendant',\n",
              " 'la',\n",
              " 'période',\n",
              " 'de',\n",
              " 'la',\n",
              " 'guerre',\n",
              " \"d'Algérie\",\n",
              " 'pour',\n",
              " 'certains',\n",
              " \"-.L'évolution\",\n",
              " 'des',\n",
              " 'choses',\n",
              " 'mettait',\n",
              " 'cela',\n",
              " 'en',\n",
              " 'cause.',\n",
              " \"L'évolution,\",\n",
              " \"c'était\",\n",
              " \"d'abord\",\n",
              " \"l'installation\",\n",
              " 'de',\n",
              " 'la',\n",
              " 'paix',\n",
              " 'en',\n",
              " 'Europe.',\n",
              " 'Et',\n",
              " 'cela',\n",
              " 'a',\n",
              " 'été',\n",
              " 'une',\n",
              " 'grande',\n",
              " 'réforme,',\n",
              " 'à',\n",
              " 'partir',\n",
              " 'du',\n",
              " 'moment',\n",
              " 'ou',\n",
              " \"l'on\",\n",
              " 'a',\n",
              " 'lancé',\n",
              " 'la',\n",
              " 'construction',\n",
              " 'européenne:',\n",
              " 'nous',\n",
              " \"n'avons\",\n",
              " 'plus',\n",
              " 'été',\n",
              " 'menacés',\n",
              " 'sur',\n",
              " 'nos',\n",
              " 'frontières.',\n",
              " 'Il',\n",
              " 'fut',\n",
              " 'un',\n",
              " 'temps',\n",
              " 'où',\n",
              " 'nous',\n",
              " 'devions',\n",
              " 'pouvoir',\n",
              " 'opposer',\n",
              " 'un',\n",
              " 'barrage',\n",
              " 'de',\n",
              " 'poitrines',\n",
              " 'aux',\n",
              " 'poitrines',\n",
              " 'allemandes',\n",
              " 'qui',\n",
              " 'risquaient',\n",
              " 'de',\n",
              " \"s'avancer\",\n",
              " 'chez',\n",
              " 'nous,',\n",
              " 'il',\n",
              " 'fallait',\n",
              " 'encore',\n",
              " 'beaucoup',\n",
              " \"d'hommes.\",\n",
              " 'La',\n",
              " 'construction',\n",
              " 'européenne',\n",
              " 'a',\n",
              " 'fait',\n",
              " 'disparaître',\n",
              " 'cette',\n",
              " 'menace.',\n",
              " 'Nous',\n",
              " 'ne',\n",
              " 'sommes',\n",
              " 'plus',\n",
              " 'menacés',\n",
              " 'à',\n",
              " 'nos',\n",
              " 'frontières:',\n",
              " 'on',\n",
              " \"n'imagine\",\n",
              " 'pas',\n",
              " \"l'Allemagne\",\n",
              " 'attaquant',\n",
              " 'la',\n",
              " 'France,',\n",
              " 'on',\n",
              " 'ne',\n",
              " \"l'image\",\n",
              " 'plus.',\n",
              " 'Donc,',\n",
              " 'en',\n",
              " 'toute',\n",
              " 'hypothèse,',\n",
              " 'la',\n",
              " 'menace',\n",
              " \"s'était\",\n",
              " 'éloignée.',\n",
              " 'Il',\n",
              " 'y',\n",
              " 'avait',\n",
              " 'aussi,',\n",
              " 'naturellement,',\n",
              " \"l'armement\",\n",
              " 'qui',\n",
              " 'avait',\n",
              " 'beaucoup',\n",
              " 'changé,',\n",
              " 'les',\n",
              " 'techniques',\n",
              " 'militaires',\n",
              " 'qui',\n",
              " \"s'étaient\",\n",
              " 'profondément',\n",
              " 'transformées.',\n",
              " \"Aujourd'hui,\",\n",
              " 'une',\n",
              " 'opération',\n",
              " 'militaire,',\n",
              " 'la',\n",
              " 'guerre',\n",
              " 'le',\n",
              " 'cas',\n",
              " 'échéant,',\n",
              " 'mobilise',\n",
              " 'des',\n",
              " 'hommes',\n",
              " 'et',\n",
              " 'des',\n",
              " 'femmes',\n",
              " 'qui',\n",
              " 'ont',\n",
              " 'une',\n",
              " 'très',\n",
              " 'haute',\n",
              " 'compétence,',\n",
              " 'qui',\n",
              " 'sont',\n",
              " 'aptes',\n",
              " 'à',\n",
              " 'utiliser',\n",
              " 'des',\n",
              " 'moyens',\n",
              " 'extraordinairement',\n",
              " 'sophistiqués,',\n",
              " 'qui',\n",
              " 'sont',\n",
              " 'totalement',\n",
              " 'disponibles',\n",
              " 'et',\n",
              " 'prêts',\n",
              " 'à',\n",
              " 'faire',\n",
              " 'mouvement',\n",
              " 'immédiatement,',\n",
              " 'et',\n",
              " 'qui',\n",
              " \"n'ont\",\n",
              " 'besoin',\n",
              " \"d'aucune\",\n",
              " 'formation',\n",
              " 'préalable',\n",
              " 'pour',\n",
              " 'le',\n",
              " \"faire.C'est\",\n",
              " 'donc',\n",
              " 'un',\n",
              " 'autre',\n",
              " 'type',\n",
              " 'de',\n",
              " 'besoin',\n",
              " 'que',\n",
              " 'nous',\n",
              " 'avons',\n",
              " 'et',\n",
              " 'qui',\n",
              " 'ne',\n",
              " 'correspond',\n",
              " 'plus',\n",
              " 'au',\n",
              " 'service',\n",
              " 'militaire',\n",
              " 'tel',\n",
              " \"qu'on\",\n",
              " 'le',\n",
              " 'concevait',\n",
              " 'avant.',\n",
              " \"C'est\",\n",
              " 'donc',\n",
              " 'une',\n",
              " 'armée',\n",
              " 'professionnelle',\n",
              " \"qu'il\",\n",
              " 'nous',\n",
              " 'faut,',\n",
              " \"c'est\",\n",
              " 'une',\n",
              " 'armée',\n",
              " 'de',\n",
              " 'métier,',\n",
              " 'ce',\n",
              " 'sont',\n",
              " 'des',\n",
              " 'vrais',\n",
              " 'professionnels.A',\n",
              " 'partir',\n",
              " 'de',\n",
              " 'cette',\n",
              " 'réflexion,',\n",
              " \"j'ai\",\n",
              " 'pensé',\n",
              " 'que',\n",
              " 'finalement,',\n",
              " 'quels',\n",
              " 'que',\n",
              " 'soient',\n",
              " 'les',\n",
              " 'arguments,',\n",
              " 'de',\n",
              " 'nature',\n",
              " 'sociale',\n",
              " 'ou',\n",
              " 'nationale',\n",
              " 'qui',\n",
              " 'plaidaient',\n",
              " 'en',\n",
              " 'faveur',\n",
              " 'du',\n",
              " 'service',\n",
              " 'militaire,',\n",
              " 'ce',\n",
              " \"n'était\",\n",
              " 'plus',\n",
              " 'adapté,',\n",
              " 'et',\n",
              " \"j'ai\",\n",
              " 'décidé',\n",
              " 'de',\n",
              " 'le',\n",
              " 'supprimer.',\n",
              " 'Mais',\n",
              " 'la',\n",
              " 'suppression',\n",
              " 'du',\n",
              " 'service',\n",
              " 'militaire,',\n",
              " 'cela',\n",
              " 'ne',\n",
              " 'veut',\n",
              " 'pas',\n",
              " 'dire',\n",
              " 'la',\n",
              " 'suppression',\n",
              " 'de',\n",
              " 'tout',\n",
              " 'lien',\n",
              " 'entre',\n",
              " 'la',\n",
              " 'jeunesse',\n",
              " 'et',\n",
              " \"l'armée.\",\n",
              " 'Il',\n",
              " 'faut',\n",
              " 'que',\n",
              " 'les',\n",
              " 'jeunes',\n",
              " 'sachent',\n",
              " 'ce',\n",
              " 'que',\n",
              " \"c'est\",\n",
              " 'que',\n",
              " 'leur',\n",
              " 'armée.',\n",
              " 'Il',\n",
              " 'faut',\n",
              " 'que',\n",
              " 'vous',\n",
              " 'le',\n",
              " 'sachiez,',\n",
              " 'parce',\n",
              " 'que',\n",
              " 'vous',\n",
              " 'êtes',\n",
              " 'dépositaires',\n",
              " 'de',\n",
              " \"l'avenir.\",\n",
              " 'On',\n",
              " \"s'imagine\",\n",
              " 'que',\n",
              " 'tout',\n",
              " 'va',\n",
              " 'bien,',\n",
              " \"qu'il\",\n",
              " \"n'y\",\n",
              " 'a',\n",
              " 'plus',\n",
              " 'de',\n",
              " 'problème.',\n",
              " 'Quand',\n",
              " 'on',\n",
              " 'est',\n",
              " 'né',\n",
              " 'dans',\n",
              " 'les',\n",
              " 'années',\n",
              " '1980,',\n",
              " 'on',\n",
              " 'a',\n",
              " 'toujours',\n",
              " 'connu',\n",
              " 'la',\n",
              " 'liberté,',\n",
              " 'la',\n",
              " 'démocratie,',\n",
              " 'les',\n",
              " 'embêtements',\n",
              " 'de',\n",
              " 'la',\n",
              " 'vie,',\n",
              " 'les',\n",
              " 'problèmes,',\n",
              " 'le',\n",
              " 'chômage,',\n",
              " 'ou',\n",
              " \"d'autres\",\n",
              " 'choses,',\n",
              " 'mais',\n",
              " 'on',\n",
              " 'a',\n",
              " 'pas',\n",
              " 'connu',\n",
              " 'la',\n",
              " 'guerre',\n",
              " 'naturellement.',\n",
              " 'Vos',\n",
              " 'grands-pères',\n",
              " \"l'ont\",\n",
              " 'connue,',\n",
              " 'pas',\n",
              " 'vous.',\n",
              " 'Cela',\n",
              " 'ne',\n",
              " 'vous',\n",
              " 'dit',\n",
              " 'rien,',\n",
              " 'et',\n",
              " 'pourtant',\n",
              " \"c'est\",\n",
              " 'en',\n",
              " 'permanence',\n",
              " 'une',\n",
              " 'menace.',\n",
              " 'La',\n",
              " 'France',\n",
              " 'a',\n",
              " 'des',\n",
              " 'intérêts',\n",
              " 'dans',\n",
              " 'le',\n",
              " 'monde,',\n",
              " 'elle',\n",
              " 'doit',\n",
              " 'le',\n",
              " 'cas',\n",
              " 'échéant',\n",
              " 'les',\n",
              " 'défendre,',\n",
              " 'et',\n",
              " 'il',\n",
              " 'faut',\n",
              " 'que',\n",
              " 'la',\n",
              " 'France',\n",
              " 'puisse',\n",
              " 'apporter',\n",
              " 'sa',\n",
              " 'contribution',\n",
              " 'militaire',\n",
              " 'pour',\n",
              " 'permettre',\n",
              " 'de',\n",
              " 'maintenir',\n",
              " 'la',\n",
              " 'paix',\n",
              " 'ici',\n",
              " 'où',\n",
              " 'là,',\n",
              " 'ou',\n",
              " 'pour',\n",
              " 'permettre',\n",
              " 'de',\n",
              " 'sauver',\n",
              " 'des',\n",
              " 'vies',\n",
              " 'quand',\n",
              " \"c'est\",\n",
              " 'nécessaire.',\n",
              " 'Lorsque',\n",
              " 'la',\n",
              " 'Bosnie',\n",
              " \"s'engage\",\n",
              " 'dans',\n",
              " 'une',\n",
              " 'opération',\n",
              " 'suicidaire,',\n",
              " 'qui',\n",
              " 'fait',\n",
              " 'des',\n",
              " 'quantités',\n",
              " 'de',\n",
              " 'morts,',\n",
              " 'il',\n",
              " 'faut',\n",
              " 'que',\n",
              " 'la',\n",
              " 'France,',\n",
              " 'comme',\n",
              " \"d'autres,\",\n",
              " 'puisse',\n",
              " 'aller',\n",
              " 'maintenir',\n",
              " 'un',\n",
              " 'peu',\n",
              " \"l'ordre\",\n",
              " 'pour',\n",
              " 'éviter',\n",
              " 'les',\n",
              " 'carnages.',\n",
              " \"C'est\",\n",
              " 'la',\n",
              " 'même',\n",
              " 'chose',\n",
              " \"aujourd'hui,\",\n",
              " 'peut-être',\n",
              " '-on',\n",
              " 'ne',\n",
              " 'sait',\n",
              " 'pas,',\n",
              " \"j'espère\",\n",
              " 'que',\n",
              " 'non-',\n",
              " 'dans',\n",
              " 'le',\n",
              " 'Kosovo.',\n",
              " 'Et',\n",
              " 'tout',\n",
              " 'cela,',\n",
              " \"c'est\",\n",
              " 'aux',\n",
              " 'portes',\n",
              " 'de',\n",
              " \"l'Europe,\",\n",
              " \"c'est\",\n",
              " 'à',\n",
              " 'moins',\n",
              " 'de',\n",
              " 'deux',\n",
              " 'heures',\n",
              " 'de',\n",
              " 'chez',\n",
              " 'nous',\n",
              " 'en',\n",
              " \"avion.Lorsqu'il\",\n",
              " 'y',\n",
              " 'a',\n",
              " 'des',\n",
              " 'problèmes,',\n",
              " 'hélas,',\n",
              " 'en',\n",
              " 'Afrique',\n",
              " 'et',\n",
              " 'cela',\n",
              " 'arrive',\n",
              " 'même',\n",
              " 'si',\n",
              " \"l'Afrique\",\n",
              " 'est',\n",
              " 'sur',\n",
              " 'la',\n",
              " 'bonne',\n",
              " 'voie,',\n",
              " 'il',\n",
              " 'faut',\n",
              " 'que',\n",
              " 'nous',\n",
              " 'ayons',\n",
              " 'la',\n",
              " 'possibilité,',\n",
              " 'en',\n",
              " 'quelques',\n",
              " 'heures,',\n",
              " \"d'aller\",\n",
              " 'récupérer',\n",
              " 'des',\n",
              " 'Français',\n",
              " 'ou',\n",
              " 'des',\n",
              " 'ressortissants',\n",
              " \"d'autres\",\n",
              " 'pays',\n",
              " 'qui',\n",
              " 'sont',\n",
              " 'là-bas',\n",
              " 'et',\n",
              " 'qui',\n",
              " 'sont',\n",
              " 'menacés',\n",
              " 'dans',\n",
              " 'leur',\n",
              " 'vie,',\n",
              " 'et',\n",
              " \"qu'il\",\n",
              " 'faut',\n",
              " 'extraire',\n",
              " 'très',\n",
              " 'rapidement.',\n",
              " 'Cela',\n",
              " 'demande',\n",
              " 'une',\n",
              " 'armée',\n",
              " 'qui',\n",
              " 'soit',\n",
              " 'non',\n",
              " 'seulement',\n",
              " 'capable',\n",
              " 'et',\n",
              " 'équipée,',\n",
              " 'mais',\n",
              " 'qui',\n",
              " 'soit',\n",
              " 'aussi',\n",
              " 'connue,',\n",
              " 'respectée',\n",
              " 'et',\n",
              " 'aimée.',\n",
              " \"D'où,\",\n",
              " 'naturellement,',\n",
              " \"l'idée\",\n",
              " 'que',\n",
              " 'les',\n",
              " 'jeunes',\n",
              " 'doivent',\n",
              " 'connaître',\n",
              " 'leur',\n",
              " 'armée,',\n",
              " 'doivent',\n",
              " 'savoir',\n",
              " 'ce',\n",
              " \"qu'elle\",\n",
              " 'est,',\n",
              " 'et',\n",
              " 'quand',\n",
              " 'je',\n",
              " 'dis',\n",
              " 'les',\n",
              " 'jeunes',\n",
              " \"c'est\",\n",
              " 'en',\n",
              " 'réalité',\n",
              " \"aujourd'hui\",\n",
              " 'les',\n",
              " 'garçons',\n",
              " 'comme',\n",
              " 'les',\n",
              " 'filles.',\n",
              " 'Alors',\n",
              " 'pour',\n",
              " 'le',\n",
              " 'moment',\n",
              " 'et',\n",
              " 'pour',\n",
              " 'des',\n",
              " 'raisons',\n",
              " 'matérielles,',\n",
              " 'techniques,',\n",
              " 'les',\n",
              " 'filles',\n",
              " 'ne',\n",
              " 'sont',\n",
              " 'pas',\n",
              " 'soumises',\n",
              " 'à',\n",
              " 'la',\n",
              " 'même',\n",
              " 'obligation',\n",
              " 'que',\n",
              " 'les',\n",
              " 'garçons,',\n",
              " 'mais',\n",
              " 'dès',\n",
              " '2000,',\n",
              " 'le',\n",
              " 'temps',\n",
              " 'de',\n",
              " 'mettre',\n",
              " 'les',\n",
              " 'choses',\n",
              " 'en',\n",
              " 'place,',\n",
              " 'les',\n",
              " 'garçons',\n",
              " 'et',\n",
              " 'les',\n",
              " 'filles',\n",
              " 'seront',\n",
              " 'sur',\n",
              " 'le',\n",
              " 'même',\n",
              " 'pied,',\n",
              " 'auront',\n",
              " 'les',\n",
              " 'mêmes',\n",
              " 'droits',\n",
              " 'et',\n",
              " 'devoirs,',\n",
              " 'les',\n",
              " 'mêmes',\n",
              " 'obligations.',\n",
              " 'Ce',\n",
              " 'sera',\n",
              " 'de',\n",
              " 'ce',\n",
              " 'point',\n",
              " 'de',\n",
              " 'vue',\n",
              " 'une',\n",
              " 'bonne',\n",
              " 'chose,',\n",
              " 'parce',\n",
              " \"qu'il\",\n",
              " \"n'y\",\n",
              " 'a',\n",
              " 'aucune',\n",
              " 'raison',\n",
              " 'que',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCJlAJ3XBsXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_text= tokenizer.tokenize(df.Texte[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QrP5mS4B145",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_text=[]\n",
        "for i in range(0,len(token_text),500):\n",
        "      split_text.append([token_text[i:i+500]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNTGQ22ZDsH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOuZKAa0By9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e69acf3d-f3d4-495d-f6ee-38dfad602335"
      },
      "source": [
        "for i in range(len(token_text)):\n",
        "      A=[token_text[i:i+500]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-5f484ff8ea20>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    split_text.append([token_text[i:i+500])\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRQ_1fQ9AyC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V15TrceNXNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the first text \n",
        "#df.Texte[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbgs39TYNqRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "392f4de4ea15400c805ee9302c4d6e8b",
            "3c066cd5ed7d4b40ba51c5a38aad1686",
            "cbb862173c314601a77db06b53680e8f",
            "ef111cc29d894ea0a9453b002f270686",
            "cccd0184e0004943af1fa3b870954a28",
            "aea8e499924c4ca6a26d57c7cbd64fa1",
            "b909758c6ffb463eb860a4a9d2aeb2ca",
            "d6725d08e2a245b0b1e684ba43de5519"
          ]
        },
        "outputId": "982d42f2-af34-40e1-f17f-3fe55751f507"
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='left') #right\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "392f4de4ea15400c805ee9302c4d6e8b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=810912, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08yLt5edOhL3",
        "colab_type": "code",
        "outputId": "61043eab-4659-4d70-d9b3-601f9c518033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Mesdames et MessieursJ'ai souhaité être aujourd'hui présente à Roissy sur l'aéroport Charles de Gaulle, pour assister au travail des agents des douanes et des services vétérinaires dans la lutte contre le trafic d'espèces sauvages et témoigner, auprès d'eux, de la détermination du gouvernement français à lutter contre ce fléau.Le trafic de plantes ou d'animaux sauvages contribue en effet de façon tout à fait dramatique et, malheureusement parfois déterminante, à la disparition des espèces tout autour de la planète.De par la masse financière qu'il représente, ce trafic compte aujourd'hui parmi les plus importants dans le monde, avec la drogue et la contrefaçon.Nos concitoyens n'en ont pas suffisamment conscience, encore aujourd'hui.A l'heure des départs en vacances de Noël, à l'heure de la détente et du bonheur partagé en famille, parfois sous d'autres latitudes, je voulais rappeler que l'insouciance en la matière peut être, pour la nature, criminelle.Importer illégalement un animal ou une plante, un produit issu d'un animal ou d'une plante (peau, dent, défense, carapace, ?ufs, etc.) c'est encourager le pillage des ressources naturelles qui consiste à prélever plus que le milieu n'est capable de produire. Cela peut se traduire par un retrait définitif de jeunes ou de reproducteurs dans le milieu naturel.C'est faire une ?uvre de destruction et de mort, le plus souvent sans en avoir conscience. Au rythme actuel beaucoup d'espèces n'y résisteront pas.Les prélèvements illégaux portant sur les espèces les plus rares, comme certaines orchidées, certains perroquets ou les tortues marines, de même que les prélèvements excessifs sur des espèces moins fragiles, comme les singes magot ou les hippocampes, conduisent à leur mise en danger ou à leur rapide disparition.Toutes les origines géographiques sont concernées mais spécialement celles d'Asie et de la zone inter-tropicale.Les touristes qui, en achetant ces produits dans les pays d'origine, en important des plantes ou des animaux vivants ou morts, ne se soucient pas de la légalité de leur provenance, peuvent, sans le vouloir, se retrouver complices d'un commerce frauduleux, dangereux pour la nature.Mon ministère vient d'éditer des documents d'information pour les touristes les prévenant de ces dangers et des risques juridiques encourus.Parallèlement, il diffuse actuellement un dépliant et une affiche destinés à sensibiliser nos concitoyens sur les mesures que mes services ont mises en place pour encadrer strictement la détention d'animaux d'espèces non domestiques. Cette réglementation nationale, qui impose notamment une traçabilité des animaux d'espèces protégées dans les élevages d'agrément, les établissements d'élevage ou de vente, les parcs zoologiques, les aquariums et les cirques, vise à protéger la biodiversité, à garantir le bien-être animal et la sécurité des personnes, mais aussi à diriger les activités d'élevage vers des programmes contribuant à la préservation de la biodiversité et à sa mise en valeur.Ces mesures nationales s'inscrivent dans un cadre plus large de lutte contre le commerce illicite des espèces sauvages : un atelier de travail européen, organisé fin octobre par la Présidence britannique, a souligné la nécessité de renforcer la communication, la coopération et la coordination entre les différents États membres de l'Union européenne pour accroître l'efficacité des services de contrôle. Je viens d'assurer mon collègue du Royaume Uni, le ministre Jim Knight, de mon entier soutien et lui ai fait part de ma volonté de sensibiliser mes services, ainsi que les autres ministres concernés, à la nécessité de les mettre en ?uvre au plus vite les recommandations de cet atelier.En 2004, les douanes françaises ont saisi 585 animaux vivants, 35 animaux naturalisés, 412 pièces d'ivoire brut ou travaillé représentant 309 kg, 1895 coquillages et coraux et 2937 autres spécimens, dont 120 kg de caviar.Il convient toutefois de rappeler que notre pays importe chaque année de nombreux animaux, plantes ou produits d'origine naturelle dont le commerce légal et encadré contribue au développement durable. La France importe ainsi plus de 13 tonnes de caviar licite et en produit plus de 17 tonnes en aquaculture. Ce caviar légal bénéficiera bientôt d'un nouveau système de traçabilité que mon ministère met actuellement au point avec les entreprises concernées.Je remercie Aéroports De Paris et Air France de s'être portés partenaires de cette opération en rendant disponibles les plaquettes d'information.Chacun d'entre nous, par un comportement responsable lors de ses déplacements professionnels ou de ses vacances peut, en refusant la complicité avec ce trafic illégal, ?uvrer activement pour sauver la nature dans le monde.(\n",
            "Tokenized:  ['▁Mesdames', '▁et', '▁Messieurs', 'J', \"'\", 'ai', '▁souhaité', '▁être', '▁aujourd', \"'\", 'hui', '▁présente', '▁à', '▁Roissy', '▁sur', '▁l', \"'\", 'aéroport', '▁Charles', '▁de', '▁Gaulle', ',', '▁pour', '▁assister', '▁au', '▁travail', '▁des', '▁agents', '▁des', '▁douane', 's', '▁et', '▁des', '▁services', '▁vétérinaire', 's', '▁dans', '▁la', '▁lutte', '▁contre', '▁le', '▁trafic', '▁d', \"'\", 'espèces', '▁sauvages', '▁et', '▁témoigner', ',', '▁auprès', '▁d', \"'\", 'eux', ',', '▁de', '▁la', '▁détermination', '▁du', '▁gouvernement', '▁français', '▁à', '▁lutter', '▁contre', '▁ce', '▁fléau', '.', 'Le', '▁trafic', '▁de', '▁plantes', '▁ou', '▁d', \"'\", 'animaux', '▁sauvages', '▁contribue', '▁en', '▁effet', '▁de', '▁façon', '▁tout', '▁à', '▁fait', '▁dramatique', '▁et', ',', '▁malheureusement', '▁parfois', '▁déterminant', 'e', ',', '▁à', '▁la', '▁disparition', '▁des', '▁espèces', '▁tout', '▁autour', '▁de', '▁la', '▁planète', '.', 'De', '▁par', '▁la', '▁masse', '▁financière', '▁qu', \"'\", 'il', '▁représente', ',', '▁ce', '▁trafic', '▁compte', '▁aujourd', \"'\", 'hui', '▁parmi', '▁les', '▁plus', '▁importants', '▁dans', '▁le', '▁monde', ',', '▁avec', '▁la', '▁drogue', '▁et', '▁la', '▁contrefaçon', '.', 'No', 's', '▁concitoyens', '▁n', \"'\", 'en', '▁ont', '▁pas', '▁suffisamment', '▁conscience', ',', '▁encore', '▁aujourd', \"'\", 'hui', '.', 'A', '▁l', \"'\", 'heure', '▁des', '▁départ', 's', '▁en', '▁vacances', '▁de', '▁Noël', ',', '▁à', '▁l', \"'\", 'heure', '▁de', '▁la', '▁détente', '▁et', '▁du', '▁bonheur', '▁partagé', '▁en', '▁famille', ',', '▁parfois', '▁sous', '▁d', \"'\", 'autres', '▁la', 'titude', 's', ',', '▁je', '▁voulais', '▁rappeler', '▁que', '▁l', \"'\", 'ins', 'ou', 'ci', 'ance', '▁en', '▁la', '▁matière', '▁peut', '▁être', ',', '▁pour', '▁la', '▁nature', ',', '▁criminelle', '.', 'Im', 'porter', '▁illégale', 'ment', '▁un', '▁animal', '▁ou', '▁une', '▁plante', ',', '▁un', '▁produit', '▁issu', '▁d', \"'\", 'un', '▁animal', '▁ou', '▁d', \"'\", 'une', '▁plante', '▁(', 'p', 'eau', ',', '▁de', 'nt', ',', '▁défense', ',', '▁carapace', ',', '▁?', 'uf', 's', ',', '▁etc', '.', ')', '▁c', \"'\", 'est', '▁encourager', '▁le', '▁pillage', '▁des', '▁ressources', '▁naturelles', '▁qui', '▁consiste', '▁à', '▁prélever', '▁plus', '▁que', '▁le', '▁milieu', '▁n', \"'\", 'est', '▁capable', '▁de', '▁produire', '.', '▁Cela', '▁peut', '▁se', '▁traduire', '▁par', '▁un', '▁retrait', '▁définitif', '▁de', '▁jeunes', '▁ou', '▁de', '▁repro', 'ducteur', 's', '▁dans', '▁le', '▁milieu', '▁naturel', '.', 'C', \"'\", 'est', '▁faire', '▁une', '▁?', 'uvre', '▁de', '▁destruction', '▁et', '▁de', '▁mort', ',', '▁le', '▁plus', '▁souvent', '▁sans', '▁en', '▁avoir', '▁conscience', '.', '▁Au', '▁rythme', '▁actuel', '▁beaucoup', '▁d', \"'\", 'espèces', '▁n', \"'\", 'y', '▁résister', 'ont', '▁pas', '.', 'Les', '▁prélèvements', '▁illégaux', '▁portant', '▁sur', '▁les', '▁espèces', '▁les', '▁plus', '▁rares', ',', '▁comme', '▁certaines', '▁', 'orchidée', 's', ',', '▁certains', '▁perroquet', 's', '▁ou', '▁les', '▁tortue', 's', '▁marine', 's', ',', '▁de', '▁même', '▁que', '▁les', '▁prélèvements', '▁excessif', 's', '▁sur', '▁des', '▁espèces', '▁moins', '▁fragiles', ',', '▁comme', '▁les', '▁singe', 's', '▁ma', 'got', '▁ou', '▁les', '▁hip', 'po', 'camp', 'es', ',', '▁conduisent', '▁à', '▁leur', '▁mise', '▁en', '▁danger', '▁ou', '▁à', '▁leur', '▁rapide', '▁disparition', '.', 'Tout', 'es', '▁les', '▁origines', '▁géographiques', '▁sont', '▁concernées', '▁mais', '▁spécialement', '▁celles', '▁d', \"'\", 'Asie', '▁et', '▁de', '▁la', '▁zone', '▁inter', '-', 'trop', 'ical', 'e', '.', 'Les', '▁touristes', '▁qui', ',', '▁en', '▁achetant', '▁ces', '▁produits', '▁dans', '▁les', '▁pays', '▁d', \"'\", 'origine', ',', '▁en', '▁important', '▁des', '▁plantes', '▁ou', '▁des', '▁animaux', '▁vivants', '▁ou', '▁morts', ',', '▁ne', '▁se', '▁souci', 'ent', '▁pas', '▁de', '▁la', '▁l', 'égalité', '▁de', '▁leur', '▁provenance', ',', '▁peuvent', ',', '▁sans', '▁le', '▁vouloir', ',', '▁se', '▁retrouver', '▁complice', 's', '▁d', \"'\", 'un', '▁commerce', '▁fr', 'aud', 'ul', 'eux', ',', '▁dangereux', '▁pour', '▁la', '▁nature', '.', 'Mon', '▁ministère', '▁vient', '▁d', \"'\", 'éditer', '▁des', '▁documents', '▁d', \"'\", 'information', '▁pour', '▁les', '▁touristes', '▁les', '▁pré', 'venant', '▁de', '▁ces', '▁dangers', '▁et', '▁des', '▁risques', '▁juridiques', '▁encouru', 's', '.', 'Par', 'all', 'èle', 'ment', ',', '▁il', '▁diffuse', '▁actuellement', '▁un', '▁dépliant', '▁et', '▁une', '▁affiche', '▁destinés', '▁à', '▁sensibiliser', '▁nos', '▁concitoyens', '▁sur', '▁les', '▁mesures', '▁que', '▁mes', '▁services', '▁ont', '▁mises', '▁en', '▁place', '▁pour', '▁en', 'cadre', 'r', '▁strictement', '▁la', '▁détention', '▁d', \"'\", 'animaux', '▁d', \"'\", 'espèces', '▁non', '▁domestiques', '.', '▁Cette', '▁réglementation', '▁nationale', ',', '▁qui', '▁impose', '▁notamment', '▁une', '▁traçabilité', '▁des', '▁animaux', '▁d', \"'\", 'espèces', '▁protégées', '▁dans', '▁les', '▁élevage', 's', '▁d', \"'\", 'agrément', ',', '▁les', '▁établissements', '▁d', \"'\", 'élevage', '▁ou', '▁de', '▁vente', ',', '▁les', '▁parcs', '▁zo', 'ologiques', ',', '▁les', '▁aquarium', 's', '▁et', '▁les', '▁cirque', 's', ',', '▁vise', '▁à', '▁protéger', '▁la', '▁biodiversité', ',', '▁à', '▁garantir', '▁le', '▁bien', '-', 'être', '▁animal', '▁et', '▁la', '▁sécurité', '▁des', '▁personnes', ',', '▁mais', '▁aussi', '▁à', '▁diriger', '▁les', '▁activités', '▁d', \"'\", 'élevage', '▁vers', '▁des', '▁programmes', '▁contribuant', '▁à', '▁la', '▁préservation', '▁de', '▁la', '▁biodiversité', '▁et', '▁à', '▁sa', '▁mise', '▁en', '▁valeur', '.', 'C', 'es', '▁mesures', '▁nationales', '▁s', \"'\", 'inscrivent', '▁dans', '▁un', '▁cadre', '▁plus', '▁large', '▁de', '▁lutte', '▁contre', '▁le', '▁commerce', '▁illicite', '▁des', '▁espèces', '▁sauvages', '▁:', '▁un', '▁atelier', '▁de', '▁travail', '▁européen', ',', '▁organisé', '▁fin', '▁octobre', '▁par', '▁la', '▁Présidence', '▁britannique', ',', '▁a', '▁souligné', '▁la', '▁nécessité', '▁de', '▁renforcer', '▁la', '▁communication', ',', '▁la', '▁coopération', '▁et', '▁la', '▁coordination', '▁entre', '▁les', '▁différents', '▁États', '▁membres', '▁de', '▁l', \"'\", 'Union', '▁européenne', '▁pour', '▁accroître', '▁l', \"'\", 'efficacité', '▁des', '▁services', '▁de', '▁contrôle', '.', '▁Je', '▁viens', '▁d', \"'\", 'assurer', '▁mon', '▁collègue', '▁du', '▁Royaume', '▁Uni', ',', '▁le', '▁ministre', '▁Jim', '▁Knight', ',', '▁de', '▁mon', '▁entier', '▁soutien', '▁et', '▁lui', '▁ai', '▁fait', '▁part', '▁de', '▁ma', '▁volonté', '▁de', '▁sensibiliser', '▁mes', '▁services', ',', '▁ainsi', '▁que', '▁les', '▁autres', '▁ministres', '▁concernés', ',', '▁à', '▁la', '▁nécessité', '▁de', '▁les', '▁mettre', '▁en', '▁?', 'uvre', '▁au', '▁plus', '▁vite', '▁les', '▁recommandations', '▁de', '▁cet', '▁atelier', '.', 'En', '▁2004,', '▁les', '▁douane', 's', '▁françaises', '▁ont', '▁saisi', '▁5', '85', '▁animaux', '▁vivants', ',', '▁35', '▁animaux', '▁', 'natural', 'isés', ',', '▁4', '12', '▁pièces', '▁d', \"'\", 'ivoire', '▁brut', '▁ou', '▁travaillé', '▁représentant', '▁3', '09', '▁kg', ',', '▁18', '95', '▁coquillage', 's', '▁et', '▁cor', 'aux', '▁et', '▁29', '37', '▁autres', '▁spécimen', 's', ',', '▁dont', '▁120', '▁kg', '▁de', '▁c', 'avi', 'ar', '.', 'Il', '▁convient', '▁toutefois', '▁de', '▁rappeler', '▁que', '▁notre', '▁pays', '▁importe', '▁chaque', '▁année', '▁de', '▁nombreux', '▁animaux', ',', '▁plantes', '▁ou', '▁produits', '▁d', \"'\", 'origine', '▁naturelle', '▁dont', '▁le', '▁commerce', '▁légal', '▁et', '▁encadré', '▁contribue', '▁au', '▁développement', '▁durable', '.', '▁La', '▁France', '▁importe', '▁ainsi', '▁plus', '▁de', '▁13', '▁tonnes', '▁de', '▁c', 'avi', 'ar', '▁', 'licite', '▁et', '▁en', '▁produit', '▁plus', '▁de', '▁17', '▁tonnes', '▁en', '▁a', 'qua', 'culture', '.', '▁Ce', '▁c', 'avi', 'ar', '▁légal', '▁bénéficier', 'a', '▁bientôt', '▁d', \"'\", 'un', '▁nouveau', '▁système', '▁de', '▁traçabilité', '▁que', '▁mon', '▁ministère', '▁met', '▁actuellement', '▁au', '▁point', '▁avec', '▁les', '▁entreprises', '▁concernées', '.', 'Je', '▁remercie', '▁Aéroport', 's', '▁De', '▁Paris', '▁et', '▁Air', '▁France', '▁de', '▁s', \"'\", 'être', '▁portés', '▁partenaires', '▁de', '▁cette', '▁opération', '▁en', '▁rendant', '▁disponibles', '▁les', '▁plaquettes', '▁d', \"'\", 'information', '.', 'Cha', 'c', 'un', '▁d', \"'\", 'entre', '▁nous', ',', '▁par', '▁un', '▁comportement', '▁responsable', '▁lors', '▁de', '▁ses', '▁déplacements', '▁professionnels', '▁ou', '▁de', '▁ses', '▁vacances', '▁peut', ',', '▁en', '▁refusant', '▁la', '▁complicité', '▁avec', '▁ce', '▁trafic', '▁illégal', ',', '▁?', 'uvre', 'r', '▁activement', '▁pour', '▁sauver', '▁la', '▁nature', '▁dans', '▁le', '▁monde', '.', '(']\n",
            "Token IDs:  [23605, 14, 19923, 655, 11, 73, 6267, 98, 405, 11, 265, 679, 15, 28658, 32, 17, 11, 4220, 2662, 8, 12493, 7, 24, 6387, 36, 225, 20, 3574, 20, 13695, 10, 14, 20, 440, 9119, 10, 29, 13, 1671, 192, 16, 3893, 18, 11, 21230, 8286, 14, 15461, 7, 843, 18, 11, 914, 7, 8, 13, 8742, 25, 754, 430, 15, 4357, 192, 44, 22643, 9, 990, 3893, 8, 2438, 47, 18, 11, 10389, 8286, 6621, 22, 340, 8, 429, 66, 15, 82, 8013, 14, 7, 3125, 610, 9711, 35, 7, 15, 13, 5374, 20, 3472, 66, 542, 8, 13, 2666, 9, 4217, 37, 13, 2269, 2903, 46, 11, 62, 1715, 7, 44, 3893, 287, 405, 11, 265, 865, 19, 40, 3111, 29, 16, 164, 7, 42, 13, 7455, 14, 13, 21611, 9, 9041, 10, 22089, 49, 11, 90, 96, 34, 3449, 1582, 7, 143, 405, 11, 265, 9, 243, 17, 11, 1130, 20, 868, 10, 22, 866, 8, 1911, 7, 15, 17, 11, 1130, 8, 13, 4796, 14, 25, 1572, 5789, 22, 401, 7, 610, 161, 18, 11, 266, 13, 9512, 10, 7, 50, 2395, 3318, 27, 17, 11, 1210, 308, 244, 1269, 22, 13, 763, 104, 98, 7, 24, 13, 696, 7, 14399, 9, 14629, 16335, 12472, 131, 23, 3831, 47, 28, 4225, 7, 23, 501, 5887, 18, 11, 59, 3831, 47, 18, 11, 70, 4225, 38, 286, 252, 7, 8, 113, 7, 1923, 7, 30624, 7, 106, 5393, 10, 7, 498, 9, 53, 60, 11, 41, 10169, 16, 27395, 20, 1388, 5000, 31, 2292, 15, 30260, 40, 27, 16, 886, 49, 11, 41, 1811, 8, 2983, 9, 683, 104, 48, 10519, 37, 23, 5103, 12720, 8, 538, 47, 8, 17271, 13891, 10, 29, 16, 886, 1649, 9, 228, 11, 41, 85, 28, 106, 7681, 8, 5876, 14, 8, 626, 7, 16, 40, 355, 112, 22, 190, 1582, 9, 277, 2112, 2989, 217, 18, 11, 21230, 49, 11, 105, 7336, 263, 34, 9, 1607, 19403, 29264, 2625, 32, 19, 3472, 19, 40, 3548, 7, 79, 787, 21, 30316, 10, 7, 420, 28728, 10, 47, 19, 14625, 10, 5546, 10, 7, 8, 93, 27, 19, 19403, 20118, 10, 32, 20, 3472, 175, 16334, 7, 79, 19, 13382, 10, 155, 10314, 47, 19, 13926, 1604, 12024, 80, 7, 21627, 15, 97, 375, 22, 2873, 47, 15, 97, 924, 5374, 9, 9595, 80, 19, 6101, 17414, 56, 9035, 65, 3473, 989, 18, 11, 8284, 14, 8, 13, 1121, 1361, 26, 8569, 6533, 35, 9, 1607, 5920, 31, 7, 22, 22045, 119, 336, 29, 19, 256, 18, 11, 870, 7, 22, 693, 20, 2438, 47, 20, 1707, 8995, 47, 2879, 7, 45, 48, 3117, 326, 34, 8, 13, 17, 4293, 8, 97, 8471, 7, 316, 7, 112, 16, 2375, 7, 48, 1008, 11708, 10, 18, 11, 59, 1599, 2542, 3679, 1137, 914, 7, 4074, 24, 13, 696, 9, 9531, 2011, 620, 18, 11, 17593, 20, 1801, 18, 11, 1070, 24, 19, 5920, 19, 790, 6846, 8, 119, 11903, 14, 20, 1712, 7974, 27108, 10, 9, 6143, 3645, 5620, 131, 7, 51, 11026, 1344, 23, 28681, 14, 28, 4772, 6111, 15, 11857, 166, 22089, 32, 19, 1546, 27, 249, 440, 96, 2506, 22, 218, 24, 22, 8412, 81, 7835, 13, 10935, 18, 11, 10389, 18, 11, 21230, 165, 12511, 9, 232, 5533, 945, 7, 31, 9164, 410, 28, 26497, 20, 1707, 18, 11, 21230, 18494, 29, 19, 15646, 10, 18, 11, 14861, 7, 19, 2743, 18, 11, 9222, 47, 8, 702, 7, 19, 8210, 11508, 11299, 7, 19, 22932, 10, 14, 19, 11674, 10, 7, 3358, 15, 1948, 13, 10551, 7, 15, 3698, 16, 72, 26, 177, 3831, 14, 13, 548, 20, 242, 7, 65, 99, 15, 7155, 19, 825, 18, 11, 9222, 224, 20, 2268, 25329, 15, 13, 13995, 8, 13, 10551, 14, 15, 77, 375, 22, 810, 9, 228, 80, 1546, 5613, 52, 11, 18709, 29, 23, 473, 40, 1071, 8, 1671, 192, 16, 1599, 17759, 20, 3472, 8286, 43, 23, 3234, 8, 225, 2208, 7, 2398, 259, 732, 37, 13, 28259, 4681, 7, 33, 6725, 13, 2966, 8, 3427, 13, 1006, 7, 13, 3599, 14, 13, 8137, 128, 19, 579, 1564, 580, 8, 17, 11, 1906, 1467, 24, 19560, 17, 11, 4001, 20, 440, 8, 963, 9, 100, 1795, 18, 11, 2688, 129, 8714, 25, 4077, 16534, 7, 16, 938, 15057, 28369, 7, 8, 129, 1821, 1237, 14, 111, 768, 82, 292, 8, 155, 1511, 8, 11857, 249, 440, 7, 163, 27, 19, 214, 7427, 5552, 7, 15, 13, 2966, 8, 19, 328, 22, 106, 7681, 36, 40, 715, 19, 6117, 8, 280, 3234, 9, 1855, 10106, 19, 13695, 10, 3717, 96, 8120, 205, 3540, 1707, 8995, 7, 1740, 1707, 21, 22997, 5575, 7, 181, 1861, 967, 18, 11, 17622, 6201, 47, 3247, 2975, 135, 3354, 3995, 7, 301, 3397, 24508, 10, 14, 3527, 483, 14, 1324, 3675, 214, 24926, 10, 7, 174, 4156, 3995, 8, 60, 9484, 848, 9, 1799, 2480, 2185, 8, 3318, 27, 127, 256, 3422, 251, 433, 8, 490, 1707, 7, 2438, 47, 336, 18, 11, 870, 2588, 174, 16, 1599, 8025, 14, 16471, 6621, 36, 499, 2028, 9, 61, 184, 3422, 163, 40, 8, 560, 5464, 8, 60, 9484, 848, 21, 23647, 14, 22, 501, 40, 8, 458, 5464, 22, 33, 3264, 13145, 9, 148, 60, 9484, 848, 8025, 2135, 55, 1679, 18, 11, 59, 281, 439, 8, 26497, 27, 129, 2011, 835, 1344, 36, 299, 42, 19, 699, 9035, 9, 1684, 5291, 19206, 10, 137, 300, 14, 2733, 184, 8, 52, 11, 177, 14878, 1626, 8, 78, 3475, 22, 7828, 1339, 19, 20100, 18, 11, 1070, 9, 10451, 216, 59, 18, 11, 1023, 63, 7, 37, 23, 2379, 1295, 298, 8, 89, 6804, 941, 47, 8, 89, 866, 104, 7, 22, 23172, 13, 10932, 42, 44, 3893, 19709, 7, 106, 7681, 81, 11449, 24, 3900, 13, 696, 29, 16, 164, 9, 413]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = df.Texte.values\n",
        "labels = df.sexe.values\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "# Apply function to our corpus\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                   )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        " \n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "outputId": "4d4563af-f2d8-41d2-c43f-98bb0972d821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Original text and transformed tensor print \n",
        "print('Original: ', texts[0])\n",
        "print('IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Mesdames et MessieursJ'ai souhaité être aujourd'hui présente à Roissy sur l'aéroport Charles de Gaulle, pour assister au travail des agents des douanes et des services vétérinaires dans la lutte contre le trafic d'espèces sauvages et témoigner, auprès d'eux, de la détermination du gouvernement français à lutter contre ce fléau.Le trafic de plantes ou d'animaux sauvages contribue en effet de façon tout à fait dramatique et, malheureusement parfois déterminante, à la disparition des espèces tout autour de la planète.De par la masse financière qu'il représente, ce trafic compte aujourd'hui parmi les plus importants dans le monde, avec la drogue et la contrefaçon.Nos concitoyens n'en ont pas suffisamment conscience, encore aujourd'hui.A l'heure des départs en vacances de Noël, à l'heure de la détente et du bonheur partagé en famille, parfois sous d'autres latitudes, je voulais rappeler que l'insouciance en la matière peut être, pour la nature, criminelle.Importer illégalement un animal ou une plante, un produit issu d'un animal ou d'une plante (peau, dent, défense, carapace, ?ufs, etc.) c'est encourager le pillage des ressources naturelles qui consiste à prélever plus que le milieu n'est capable de produire. Cela peut se traduire par un retrait définitif de jeunes ou de reproducteurs dans le milieu naturel.C'est faire une ?uvre de destruction et de mort, le plus souvent sans en avoir conscience. Au rythme actuel beaucoup d'espèces n'y résisteront pas.Les prélèvements illégaux portant sur les espèces les plus rares, comme certaines orchidées, certains perroquets ou les tortues marines, de même que les prélèvements excessifs sur des espèces moins fragiles, comme les singes magot ou les hippocampes, conduisent à leur mise en danger ou à leur rapide disparition.Toutes les origines géographiques sont concernées mais spécialement celles d'Asie et de la zone inter-tropicale.Les touristes qui, en achetant ces produits dans les pays d'origine, en important des plantes ou des animaux vivants ou morts, ne se soucient pas de la légalité de leur provenance, peuvent, sans le vouloir, se retrouver complices d'un commerce frauduleux, dangereux pour la nature.Mon ministère vient d'éditer des documents d'information pour les touristes les prévenant de ces dangers et des risques juridiques encourus.Parallèlement, il diffuse actuellement un dépliant et une affiche destinés à sensibiliser nos concitoyens sur les mesures que mes services ont mises en place pour encadrer strictement la détention d'animaux d'espèces non domestiques. Cette réglementation nationale, qui impose notamment une traçabilité des animaux d'espèces protégées dans les élevages d'agrément, les établissements d'élevage ou de vente, les parcs zoologiques, les aquariums et les cirques, vise à protéger la biodiversité, à garantir le bien-être animal et la sécurité des personnes, mais aussi à diriger les activités d'élevage vers des programmes contribuant à la préservation de la biodiversité et à sa mise en valeur.Ces mesures nationales s'inscrivent dans un cadre plus large de lutte contre le commerce illicite des espèces sauvages : un atelier de travail européen, organisé fin octobre par la Présidence britannique, a souligné la nécessité de renforcer la communication, la coopération et la coordination entre les différents États membres de l'Union européenne pour accroître l'efficacité des services de contrôle. Je viens d'assurer mon collègue du Royaume Uni, le ministre Jim Knight, de mon entier soutien et lui ai fait part de ma volonté de sensibiliser mes services, ainsi que les autres ministres concernés, à la nécessité de les mettre en ?uvre au plus vite les recommandations de cet atelier.En 2004, les douanes françaises ont saisi 585 animaux vivants, 35 animaux naturalisés, 412 pièces d'ivoire brut ou travaillé représentant 309 kg, 1895 coquillages et coraux et 2937 autres spécimens, dont 120 kg de caviar.Il convient toutefois de rappeler que notre pays importe chaque année de nombreux animaux, plantes ou produits d'origine naturelle dont le commerce légal et encadré contribue au développement durable. La France importe ainsi plus de 13 tonnes de caviar licite et en produit plus de 17 tonnes en aquaculture. Ce caviar légal bénéficiera bientôt d'un nouveau système de traçabilité que mon ministère met actuellement au point avec les entreprises concernées.Je remercie Aéroports De Paris et Air France de s'être portés partenaires de cette opération en rendant disponibles les plaquettes d'information.Chacun d'entre nous, par un comportement responsable lors de ses déplacements professionnels ou de ses vacances peut, en refusant la complicité avec ce trafic illégal, ?uvrer activement pour sauver la nature dans le monde.(\n",
            "IDs: tensor([    5, 23605,    14, 19923,   655,    11,    73,  6267,    98,   405,\n",
            "           11,   265,   679,    15, 28658,    32,    17,    11,  4220,  2662,\n",
            "            8, 12493,     7,    24,  6387,    36,   225,    20,  3574,    20,\n",
            "        13695,    10,    14,    20,   440,  9119,    10,    29,    13,  1671,\n",
            "          192,    16,  3893,    18,    11, 21230,  8286,    14, 15461,     7,\n",
            "          843,    18,    11,   914,     7,     8,    13,  8742,    25,   754,\n",
            "          430,    15,  4357,   192,    44, 22643,     9,   990,  3893,     8,\n",
            "         2438,    47,    18,    11, 10389,  8286,  6621,    22,   340,     8,\n",
            "          429,    66,    15,    82,  8013,    14,     7,  3125,   610,  9711,\n",
            "           35,     7,    15,    13,  5374,    20,  3472,    66,   542,     8,\n",
            "           13,  2666,     9,  4217,    37,    13,  2269,  2903,    46,    11,\n",
            "           62,  1715,     7,    44,  3893,   287,   405,    11,   265,   865,\n",
            "           19,    40,  3111,    29,    16,   164,     7,    42,    13,  7455,\n",
            "           14,    13, 21611,     9,  9041,    10, 22089,    49,    11,    90,\n",
            "           96,    34,  3449,  1582,     7,   143,   405,    11,   265,     9,\n",
            "          243,    17,    11,  1130,    20,   868,    10,    22,   866,     8,\n",
            "         1911,     7,    15,    17,    11,  1130,     8,    13,  4796,    14,\n",
            "           25,  1572,  5789,    22,   401,     7,   610,   161,    18,    11,\n",
            "          266,    13,  9512,    10,     7,    50,  2395,  3318,    27,    17,\n",
            "           11,  1210,   308,   244,  1269,    22,    13,   763,   104,    98,\n",
            "            7,    24,    13,   696,     7, 14399,     9, 14629, 16335, 12472,\n",
            "          131,    23,  3831,    47,    28,  4225,     7,    23,   501,  5887,\n",
            "           18,    11,    59,  3831,    47,    18,    11,    70,  4225,    38,\n",
            "          286,   252,     7,     8,   113,     7,  1923,     7, 30624,     7,\n",
            "          106,  5393,    10,     7,   498,     9,    53,    60,    11,    41,\n",
            "        10169,    16, 27395,    20,  1388,  5000,    31,  2292,    15, 30260,\n",
            "           40,    27,    16,   886,    49,    11,    41,  1811,     8,  2983,\n",
            "            9,   683,   104,    48, 10519,    37,    23,  5103, 12720,     8,\n",
            "          538,    47,     8, 17271, 13891,    10,    29,    16,   886,  1649,\n",
            "            9,   228,    11,    41,    85,    28,   106,  7681,     8,  5876,\n",
            "           14,     8,   626,     7,    16,    40,   355,   112,    22,   190,\n",
            "         1582,     9,   277,  2112,  2989,   217,    18,    11, 21230,    49,\n",
            "           11,   105,  7336,   263,    34,     9,  1607, 19403, 29264,  2625,\n",
            "           32,    19,  3472,    19,    40,  3548,     7,    79,   787,    21,\n",
            "        30316,    10,     7,   420, 28728,    10,    47,    19, 14625,    10,\n",
            "         5546,    10,     7,     8,    93,    27,    19, 19403, 20118,    10,\n",
            "           32,    20,  3472,   175, 16334,     7,    79,    19, 13382,    10,\n",
            "          155, 10314,    47,    19, 13926,  1604, 12024,    80,     7, 21627,\n",
            "           15,    97,   375,    22,  2873,    47,    15,    97,   924,  5374,\n",
            "            9,  9595,    80,    19,  6101, 17414,    56,  9035,    65,  3473,\n",
            "          989,    18,    11,  8284,    14,     8,    13,  1121,  1361,    26,\n",
            "         8569,  6533,    35,     9,  1607,  5920,    31,     7,    22, 22045,\n",
            "          119,   336,    29,    19,   256,    18,    11,   870,     7,    22,\n",
            "          693,    20,  2438,    47,    20,  1707,  8995,    47,  2879,     7,\n",
            "           45,    48,  3117,   326,    34,     8,    13,    17,  4293,     8,\n",
            "           97,  8471,     7,   316,     7,   112,    16,  2375,     7,    48,\n",
            "         1008, 11708,    10,    18,    11,    59,  1599,  2542,  3679,  1137,\n",
            "          914,     7,  4074,    24,    13,   696,     9,  9531,  2011,   620,\n",
            "           18,    11, 17593,    20,  1801,    18,    11,  1070,    24,    19,\n",
            "         5920,    19,   790,  6846,     8,   119, 11903,    14,    20,     6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OPWOx2-FG7g",
        "colab_type": "code",
        "outputId": "ada0d46c-3a04-4c5d-dd57-8bd428669d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFc6MW5df566",
        "colab_type": "text"
      },
      "source": [
        "#### Train and validation dataset construction \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y3MT9B-gAHU",
        "colab_type": "code",
        "outputId": "2d4f759c-5b2a-45dd-983b-ea0f097b45ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine all above\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Let's create a 80-20 train / validation dataset \n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('We have {} training samples'.format(train_size))\n",
        "print('We have {} validation samples'.format(val_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 2000 training samples\n",
            "We have 500 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF9rj3AWgynY",
        "colab_type": "text"
      },
      "source": [
        "In order to save on memory we use the convenient DataLoader of pytorch.utils "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr-fhDIQgAFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# We set the size of the batch lower than what is usually set (16 of 32)\n",
        "batch_size = 4\n",
        "\n",
        "# We create data loaders for the train and validation dataset. \n",
        "train_dataloader = DataLoader(\n",
        "            train_set,  # The training samples.\n",
        "            sampler = RandomSampler(train_set), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "            val_set, # The validation samples.\n",
        "            sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poTTEJX1hoUK",
        "colab_type": "text"
      },
      "source": [
        "### CamemBERT Sequence Classification model tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VeJI0lDwf",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MPOVB7iRcl",
        "colab_type": "text"
      },
      "source": [
        "We will finally build up our model. We will use the  CamemBERT model for sequence classification which includes a special top layer designed for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHhHzjKgAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing from transformers\n",
        "from transformers import CamembertForSequenceClassification, CamembertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMdM-QqgAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "# Ici je ne suis pas sure pour le 'cased' ou pas (je crois que oui)\n",
        "gender_model1 = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKynoykf_9y",
        "colab_type": "code",
        "outputId": "3cd9b0d2-8ae0-4ff4-e292-840efe53a3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We run the model on the colab GPU \n",
        "gender_model1.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWyHWg5xlBck",
        "colab_type": "text"
      },
      "source": [
        "Optimizers and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go-uY70Mloqv",
        "colab_type": "text"
      },
      "source": [
        "We will choose the AdamW optimizer and set for this first model the learning rate and the epsilon to default. At the batch is little we might want to increase the learning rate a bit from what is usually used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyP0__vf_7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#Implements Adam algorithm with weight decay fix.\n",
        "optimizer = AdamW(gender_model1.parameters(),\n",
        "                  lr = 5e-5, # Adaptative (yes i think)\n",
        "                  eps = 1e-8 # prevent division by 0 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYr-iTzVmXZu",
        "colab_type": "text"
      },
      "source": [
        "We fiw the number of epochs to 4\n",
        "We also configure the learning rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ1ymlmqf_4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# set number of epochs\n",
        "epochs = 5\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgwmHirxnEie",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the training and validation loop \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6rIz4UnLwq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDt2ZElwcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZt12wxqh7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQxSQsQxm6mv",
        "colab_type": "code",
        "outputId": "3b9f2db3-3a4f-4be2-ee35-d887bb3ac3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "# https://github.com/huggingface/transformers \n",
        "# https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L404  \n",
        "\n",
        "import random\n",
        "# Let's put a seed to make this result reproducible \n",
        "seed=2020\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# We want to evaluate the training phase \n",
        "training_stats = []\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "  print('Training starts')\n",
        "\n",
        "  \n",
        "\n",
        "  ################################### TRAINING ################################\n",
        "\n",
        "  # Set the train loss for the epoch to 0 \n",
        "  total_train_loss = 0\n",
        "\n",
        "  #Put the model in training \n",
        "  gender_model1.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Cpy the 3 batch to GPU \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Clear gradients \n",
        "    gender_model1.zero_grad() \n",
        "    \n",
        "    #return loss and logits\n",
        "    loss, logits = gender_model1(b_input_ids, \n",
        "                         token_type_ids=None, \n",
        "                         attention_mask=b_input_mask, \n",
        "                         labels=b_labels) \n",
        "    \n",
        "    # Accumulate training loss for all batches \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Backward to calculate gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Prevent exploding gradients problem \n",
        "    torch.nn.utils.clip_grad_norm_(gender_model1.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters \n",
        "    optimizer.step()\n",
        "\n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "  #Calculate the average training loss over all batches  \n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print('')\n",
        "  print('And now, validation STARTS')\n",
        "\n",
        "  ###################### VALIDATION #############################\n",
        "\n",
        "  # Put model in evaluation mode \n",
        "  gender_model1.eval()\n",
        "\n",
        "  # Set statistics to 0\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  nb_eval_steps = 0\n",
        "\n",
        "  # Confusion matrix ?\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "     \n",
        "     # We don't care about gradients for eval\n",
        "    with torch.no_grad(): \n",
        "      (loss, logits) = gender_model1(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "      # Move logits and labels to CPU \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Confusion matrix ?\n",
        "    val_batch_preds = np.argmax(logits, axis=1)\n",
        "    val_batch_labels = label_ids\n",
        "    predictions.extend(val_batch_preds)\n",
        "    true_labels.extend(val_batch_labels)\n",
        "\n",
        "\n",
        "\n",
        "      # Accumulation accuracy for all batch\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    #Final accuracy on all batch\n",
        "  avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #Final loss over all batch\n",
        "  avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "  # confusion matrix ? \n",
        "  pred_tags = [i for i in predictions]\n",
        "  valid_tags = [i for i in true_labels]\n",
        "\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': ep + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Done !\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 5 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.56\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.64\n",
            "===========Starting Epoch 2 / 5 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.44\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.76\n",
            "  Validation Loss: 0.60\n",
            "===========Starting Epoch 3 / 5 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.31\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.95\n",
            "===========Starting Epoch 4 / 5 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.18\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.76\n",
            "  Validation Loss: 1.09\n",
            "===========Starting Epoch 5 / 5 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.07\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.76\n",
            "  Validation Loss: 1.30\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUApHR6VH5kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqfBkNDKVxW",
        "colab_type": "code",
        "outputId": "78b7ab5d-8bf7-42bf-f784-3be50aced4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "confusion_matrix(valid_tags, pred_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[188,  62],\n",
              "       [ 60, 190]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKUBFeBAJTOK",
        "colab_type": "code",
        "outputId": "9b45aec6-7c3f-4a2e-e397-0ab8524feb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_mat = confusion_matrix(valid_tags, pred_tags)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGmCAYAAACUbzs0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMA4BCAAAGCc/wfaxAbd7Eoy2gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KceMm2E-0dF1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We can't look at accuracy with confidence. Indeed, our sample is really unbalanced and thus classifying all text as male would already give a 0.75 accuracy. This is exactly what happens here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PviHO7vHm65o",
        "colab_type": "code",
        "outputId": "cfe96679-d8bd-45b0-a706-3435de0c7798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.561435</td>\n",
              "      <td>0.643202</td>\n",
              "      <td>0.768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.444754</td>\n",
              "      <td>0.595550</td>\n",
              "      <td>0.758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.314224</td>\n",
              "      <td>0.951572</td>\n",
              "      <td>0.776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.179277</td>\n",
              "      <td>1.091080</td>\n",
              "      <td>0.762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.073712</td>\n",
              "      <td>1.301599</td>\n",
              "      <td>0.756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.\n",
              "epoch                                           \n",
              "1           0.561435     0.643202          0.768\n",
              "2           0.444754     0.595550          0.758\n",
              "3           0.314224     0.951572          0.776\n",
              "4           0.179277     1.091080          0.762\n",
              "5           0.073712     1.301599          0.756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJY9ZzP2JwK",
        "colab_type": "code",
        "outputId": "f39bca27-ca4a-4976-ce72-7e03ee3445d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiiTGUExERERFJjKGciIiIiEhi/x8iWZeTScrVmwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGOO8vm3jwL",
        "colab_type": "text"
      },
      "source": [
        "Our model is completely over fitting after 2 epochs !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdmReVji0KF5",
        "colab_type": "text"
      },
      "source": [
        "RESTE A FAIRE A VOIR AVEC MELCHIOR \n",
        "- optimizer\n",
        "- number of batch / sample / epochs \n",
        "- change criterion \n",
        "- accuracy replace with F1 score\n",
        "- plot predictions and graph \n",
        "- improve loop ? \n",
        "- Make evaluation on a test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2heoobpm69S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5oSWe6nm63Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqcRkgeSm60n",
        "colab_type": "code",
        "outputId": "103fcb28-f464-4e65-8b5f-41bb710cac79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "@article{Wolf2019HuggingFacesTS,\n",
        "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
        "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
        "  journal={ArXiv},\n",
        "  year={2019},\n",
        "  volume={abs/1910.03771}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-c848a20e4352>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    @article{Wolf2019HuggingFacesTS,\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2xLw0mY6tzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}